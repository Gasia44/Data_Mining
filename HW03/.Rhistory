data_scaleWDI<-scale(WDI[,2:9])
View(data_scaleWDI)
View(data_scaleWDI)
View(WDI)
View(WDI)
data_scale_WDI<-data.frame(Series=data$Series, as.data.frame(data_scaleWDI))
data_scaleWDI<-scale(WDI[,])
data_scale_WDI<-data.frame(as.data.frame(data_scaleWDI))
data_scale_WDI<-data.frame(Series=WDI$Series, as.data.frame(data_scaleWDI))
fviz_nbclust(WDI[,2:9], hcut, method = "silhouette",hc_method = "complete")
fviz_nbclust(GCI[,2:13], hcut, method = "silhouette",hc_method = "complete")
fviz_nbclust(WDI[,2:9], hcut, method = "silhouette",hc_method = "complete")
Soccer<-read.csv("Soccer.csv")
set.seed(5)
kmeans_Soccer1<-kmeans(Soccer[,5:42],2)
set.seed(5)
kmeans_Soccer2<-kmeans(Soccer[,5:42],3)
set.seed(5)
kmeans_Soccer3<-kmeans(Soccer[,5:42],4)
set.seed(5)
kmeans_Soccer4<-kmeans(Soccer[,5:42],5)
set.seed(5)
kmeans_Soccer5<-kmeans(Soccer[,5:42],6)
# 12. Plot the clusters for all the trials (numbers of clusters).
# Do you see any similarity in both graphs?
# Hint: you may find useful package fpc and function called plotcluster. (3)
#install.packages("fpc")
library(fpc)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer2$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer3$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer4$cluster)
Soccer$kmean_Soccer<-kmeans_Soccer2$cluster
soccer_data<-aggregate(Soccer[,5:42], by=list(Soccer$kmean_Soccer), FUN="mean", na.rm=TRUE)
View(soccer_data)
scale_soccer<-scale(Soccer[,5:42])
data_scale_soccer<-data.frame(Soccer[,1:4],as.data.frame(scale_soccer))
distance_scale_soccer<-dist(data_scale_soccer[5:42],method="euclidean")
clust_soccer<-hclust(distance_scale_soccer)
Soccer$hclust2<-cutree(clust_soccer,k=2)
Soccer$hclust3<-cutree(clust_soccer,k=3)
Soccer$hclust4<-cutree(clust_soccer,k=4)
Soccer$hclust5<-cutree(clust_soccer,k=5)
# 15. For all the trials, again create plots and try to find patterns.
# Describe the patterns that you noticed. (2)
plotcluster( Soccer[,5:42], Soccer$hclust2)
plotcluster( Soccer[,5:42], Soccer$hclust3)
plotcluster( Soccer[,5:42], Soccer$hclust2)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer2$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer3$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer4$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer2$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer3$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer4$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer5$cluster)
#in every plot we have one or more clusters at left side and the same for right
#we don't have data in between
#we have two groups, one group is being devided when we increase
#the number of clusters, and the other half stay the same
# 13. Aggregate the means for all the variables for one of
# the results that you got in question 2. Give comments about the clusters. (2)
#let me take the one with 3 clusters,because
Soccer$kmean_Soccer<-kmeans_Soccer2$cluster
soccer_data<-aggregate(Soccer[,5:42], by=list(Soccer$kmean_Soccer), FUN="mean", na.rm=TRUE)
#Cluster 3 has football players who are the tallest and older and
#they weight more than the other two clusters, since they are older
#than others, their crossing skill is the least and also their heading
#and shooting accurancy is low , they also lack stamina and their
#goal keeper skills are low, however their strength is on average
#in other words: Cluster 3 players are the oldest that have the lowest
#attributions(the skills that are given)
#cluster 2 and cluster 1 have some similarities, their players
#are younger than cluster 3 and they have more abilities
#However cluster 1 represents teh best players, best shhoting skills, dribbling,
#curves and many other skills
#wherease we can define cluster 2 as average players
# 14. Now perform hierarchical clustering. Again try with several number of clusters. (2)
scale_soccer<-scale(Soccer[,5:42])
data_scale_soccer<-data.frame(Soccer[,1:4],as.data.frame(scale_soccer))
distance_scale_soccer<-dist(data_scale_soccer[5:42],method="euclidean")
clust_soccer<-hclust(distance_scale_soccer)
Soccer$hclust2<-cutree(clust_soccer,k=2)
Soccer$hclust3<-cutree(clust_soccer,k=3)
Soccer$hclust4<-cutree(clust_soccer,k=4)
Soccer$hclust5<-cutree(clust_soccer,k=5)
# 15. For all the trials, again create plots and try to find patterns.
# Describe the patterns that you noticed. (2)
plotcluster( Soccer[,5:42], Soccer$hclust2)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
Soccer$hclust2<-cutree(clust_soccer,k=2)
Soccer$hclust3<-cutree(clust_soccer,k=3)
plotcluster( Soccer[,5:42], Soccer$hclust2)
plotcluster( Soccer[,5:42], Soccer$hclust3)
soccer_data<-aggregate(Soccer[,5:42], by=list(Soccer$kmean_Soccer), FUN="mean", na.rm=TRUE)
plotcluster( Soccer[,5:42], kmeans_Soccer2$cluster)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 2)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 3)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 4)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 6)
plot(clust_soccer, hang=-1,labels=F)
rect.hclust(clust_soccer, 8)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 3)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 4)
Club_data<-aggregate(Soccer[,5:42],by=list(Soccer$team_long_name),FUN="mean", na.rm=T)
club_scale<-as.data.frame(scale(Club_data[,-1]))
dist_club_scale<-dist(club_scale[,-1], method="euclidean")
clust_club<-hclust(dist_club_scale)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,3)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,4)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,5)
Soccer$cluster_membership<-cutree(clust_club,3)
Club_data<-aggregate(Soccer[,5:42],by=list(Soccer$team_long_name),FUN="mean", na.rm=T)
club_scale<-as.data.frame(scale(Club_data[,-1]))
dist_club_scale<-dist(club_scale[,-1], method="euclidean")
clust_club<-hclust(dist_club_scale)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,3)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,4)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,5)
Soccer$cluster_membership<-cutree(clust_club,3)
Soccer$cluster_membership<-cutree(clust_club,k=3)
Soccer<-read.csv("Soccer.csv")
set.seed(5)
kmeans_Soccer1<-kmeans(Soccer[,5:42],2)
set.seed(5)
kmeans_Soccer2<-kmeans(Soccer[,5:42],3)
set.seed(5)
kmeans_Soccer3<-kmeans(Soccer[,5:42],4)
set.seed(5)
kmeans_Soccer4<-kmeans(Soccer[,5:42],5)
set.seed(5)
kmeans_Soccer5<-kmeans(Soccer[,5:42],6)
# 12. Plot the clusters for all the trials (numbers of clusters).
# Do you see any similarity in both graphs?
# Hint: you may find useful package fpc and function called plotcluster. (3)
#install.packages("fpc")
library(fpc)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer2$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer3$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer4$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer5$cluster)
#in every plot we have two density data on the two sides and we don't
#have data in between
#we have two groups, one group is being devided when we increase
#the number of clusters, and the other half stay the same
# 13. Aggregate the means for all the variables for one of
# the results that you got in question 2. Give comments about the clusters. (2)
#let me take the one with 3 clusters.
Soccer$kmean_Soccer<-kmeans_Soccer2$cluster
soccer_data<-aggregate(Soccer[,5:42], by=list(Soccer$kmean_Soccer), FUN="mean", na.rm=TRUE)
#View(soccer_data)
#Cluster 3 has football players who are the tallest and oldest and
#they weight more than the other two clusters, since they are older
#than others, their crossing skill is the least and also their heading
#and shooting accurancy is low , they also lack stamina and their
#goal keeper skills are low, however their strength is on average
#in other words: Cluster 3 players are the oldest that have the lowest
#attributions(the skills that are given)
#cluster 2 and cluster 1 have some similarities, their players
#are younger than cluster 3 and they have more abilities
#However cluster 1 represents the best players, best shoting skills, dribbling,
#curves and many other skills
#wherease we can define cluster 2 as average players
# 14. Now perform hierarchical clustering. Again try with several number of clusters. (2)
scale_soccer<-scale(Soccer[,5:42])
data_scale_soccer<-data.frame(Soccer[,1:4],as.data.frame(scale_soccer))
distance_scale_soccer<-dist(data_scale_soccer[5:42],method="euclidean")
clust_soccer<-hclust(distance_scale_soccer)
Soccer$hclust2<-cutree(clust_soccer,k=2)
Soccer$hclust3<-cutree(clust_soccer,k=3)
Soccer$hclust4<-cutree(clust_soccer,k=4)
Soccer$hclust5<-cutree(clust_soccer,k=5)
# 15. For all the trials, again create plots and try to find patterns.
# Describe the patterns that you noticed. (2)
plotcluster( Soccer[,5:42], Soccer$hclust2)
plotcluster( Soccer[,5:42], Soccer$hclust3)
plotcluster( Soccer[,5:42], Soccer$hclust5)
plotcluster( Soccer[,5:42], Soccer$hclust4)
#Those are almost the same as we did before using kmeans approach
#the pattern is that we have two groups (starting by 2 clusers),
#one group is being devided when we increase the number of clusters,
#and the other half stay the same
#Here are also the hierarchy plots:
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 2)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 3)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 4)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 6)
plot(clust_soccer, hang=-1,labels=F)
rect.hclust(clust_soccer, 8)
#The same pattern i can also say on these plots:
#The left most cluster is fixed, and the right is being devided
#into clusters when we increase the number of clusters
# 16. Based on the patterns and similarities between the graphs that you
# noticed while performing clustering with kmeans and hierarchical
# clustering, come up with an optimal number of clusters. Choose
# either of the clustering methods, and assign clusters to each of
# the cases in the soccer dataset. (2)
#The optimal number is 3, because looking to the plot, we can see that when we
#take 4 clusters, the distance of the data between two clusters is
#narrowing (from the plot using fpc)
#Since i put before in my data the cluster memberships, i just need to
#remove what i don't want, and keep the optimal one
Soccer$hclust2<-NULL
Soccer$hclust4<-NULL
Soccer$hclust5<-NULL
# 17. Aggregate the average data for all the variables for the number
# of the clusters that you chose in question 7. What are the differences
# between those clusters? Try to give a general description for 5-6 of them. (2)
data_soccer<-aggregate(Soccer[,5:42], by=list(Soccer$hclust3), FUN="mean", na.rm=T)
#View(data_soccer)
#Cluster 1: It represents the players that are young, have a big potential,
#           are not very strength as they are still young, however
#           they have a good agility, as they are young take risks and
#           during the game without thinking. They can perfectly control
#Cluster 2: It represents average age players, with average potential,
#           who are very strong(as they were younger, they practiced a lot
#           to become stronger, they have a average agility, as they started
#           as well as dribble, volly and short pass.
#Cluster 3:It represents the oldest players, with average potential,
#           to play more by thinking, they can control the ball on average
#           the ball, and they are good at dribbling, volleys and short passing.
#           who are average in strength, with very low agility as they
#           became very wise throughout the years and can't take risks,
#           they are bad at controlling the ball, also in dribble,
#In summary we have three clusters: Good, Average, Bad
# 18. Pick a player from each of those clusters. Describe those players in terms of
#           volly and short pass.
# their affiliation to the clusters. What are the similarities and differences between them? (1)
###??????
# 19. Now aggregate the data in a way that you end up with dataframe
# where each row represents a club. Now run clustering on that dataframe
# (use whichever method you prefer.) Aggregate the average values for
# each of the clusters. Again, choose 5-6 variables, compare the clubs
# based on them, and try to give names to the clusters.
Club_data<-aggregate(Soccer[,5:42],by=list(Soccer$team_long_name),FUN="mean", na.rm=T)
club_scale<-as.data.frame(scale(Club_data[,-1]))
dist_club_scale<-dist(club_scale[,-1], method="euclidean")
clust_club<-hclust(dist_club_scale)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,3)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,4)
plot(clust_club,hang=-1, labels=F)
rect.hclust(clust_club,5)
Soccer$cluster_membership<-cutree(clust_club,k=3)
Soccer$cluster_membership<-cutree(clust_club, k=3)
rect.hclust(clust_club,3)
Soccer$cluster_membership<-cutree(clust_club, k=3)
Soccer$cluster_membership<-cutree(clust_club, k=3)
Club_data$cluster_membership<-cutree(clust_club, k=3)
data_soccer_club<-aggregate(Club_data[,-1],by=list(Club_data$cluster_membership), FUN="mean", na.rm=T)
View(data_soccer_club)
View(Soccer[850,])
Soccer<-read.csv("Soccer.csv")
set.seed(5)
kmeans_Soccer1<-kmeans(Soccer[,5:42],2)
set.seed(5)
kmeans_Soccer2<-kmeans(Soccer[,5:42],3)
set.seed(5)
kmeans_Soccer3<-kmeans(Soccer[,5:42],4)
set.seed(5)
kmeans_Soccer4<-kmeans(Soccer[,5:42],5)
set.seed(5)
kmeans_Soccer5<-kmeans(Soccer[,5:42],6)
# 12. Plot the clusters for all the trials (numbers of clusters).
# Do you see any similarity in both graphs?
# Hint: you may find useful package fpc and function called plotcluster. (3)
#install.packages("fpc")
library(fpc)
plotcluster( Soccer[,5:42], kmeans_Soccer1$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer2$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer3$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer4$cluster)
plotcluster( Soccer[,5:42], kmeans_Soccer5$cluster)
#in every plot we have two density data on the two sides and we don't
#have data in between
#we have two groups, one group is being devided when we increase
#the number of clusters, and the other half stay the same
# 13. Aggregate the means for all the variables for one of
# the results that you got in question 2. Give comments about the clusters. (2)
#let me take the one with 3 clusters.
Soccer$kmean_Soccer<-kmeans_Soccer2$cluster
soccer_data<-aggregate(Soccer[,5:42], by=list(Soccer$kmean_Soccer), FUN="mean", na.rm=TRUE)
#View(soccer_data)
#Cluster 3 has football players who are the tallest and oldest and
#they weight more than the other two clusters, since they are older
#than others, their crossing skill is the least and also their heading
#and shooting accurancy is low , they also lack stamina and their
#goal keeper skills are low, however their strength is on average
#in other words: Cluster 3 players are the oldest that have the lowest
#attributions(the skills that are given)
#cluster 2 and cluster 1 have some similarities, their players
#are younger than cluster 3 and they have more abilities
#However cluster 1 represents the best players, best shoting skills, dribbling,
#curves and many other skills
#wherease we can define cluster 2 as average players
# 14. Now perform hierarchical clustering. Again try with several number of clusters. (2)
scale_soccer<-scale(Soccer[,5:42])
data_scale_soccer<-data.frame(Soccer[,1:4],as.data.frame(scale_soccer))
distance_scale_soccer<-dist(data_scale_soccer[5:42],method="euclidean")
clust_soccer<-hclust(distance_scale_soccer)
Soccer$hclust2<-cutree(clust_soccer,k=2)
Soccer$hclust3<-cutree(clust_soccer,k=3)
Soccer$hclust4<-cutree(clust_soccer,k=4)
Soccer$hclust5<-cutree(clust_soccer,k=5)
# 15. For all the trials, again create plots and try to find patterns.
# Describe the patterns that you noticed. (2)
plotcluster( Soccer[,5:42], Soccer$hclust2)
plotcluster( Soccer[,5:42], Soccer$hclust3)
plotcluster( Soccer[,5:42], Soccer$hclust4)
plotcluster( Soccer[,5:42], Soccer$hclust5)
#Those are almost the same as we did before using kmeans approach
#the pattern is that we have two groups (starting by 2 clusers),
#one group is being devided when we increase the number of clusters,
#and the other half stay the same
#Here are also the hierarchy plots:
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 2)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 3)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 4)
plot(clust_soccer, hang=-1, labels=F)
rect.hclust(clust_soccer, 6)
plot(clust_soccer, hang=-1,labels=F)
rect.hclust(clust_soccer, 8)
#The same pattern i can also say on these plots:
#The left most cluster is fixed, and the right is being devided
#into clusters when we increase the number of clusters
# 16. Based on the patterns and similarities between the graphs that you
# noticed while performing clustering with kmeans and hierarchical
# clustering, come up with an optimal number of clusters. Choose
# either of the clustering methods, and assign clusters to each of
# the cases in the soccer dataset. (2)
#The optimal number is 3, because looking to the plot, we can see that when we
#take 4 clusters, the distance of the data between two clusters is
#narrowing (from the plot using fpc)
#Since i put before in my data the cluster memberships, i just need to
#remove what i don't want, and keep the optimal one
Soccer$hclust2<-NULL
Soccer$hclust4<-NULL
Soccer$hclust5<-NULL
# 17. Aggregate the average data for all the variables for the number
# of the clusters that you chose in question 7. What are the differences
# between those clusters? Try to give a general description for 5-6 of them. (2)
data_soccer<-aggregate(Soccer[,5:42], by=list(Soccer$hclust3), FUN="mean", na.rm=T)
#View(data_soccer)
#Cluster 1: It represents the players that are young, have a big potential,
#           are not very strength as they are still young, however
#           they have a good agility, as they are young take risks and
#           during the game without thinking. They can perfectly control
#           the ball, and they are good at dribbling, volleys and short passing.
#Cluster 2: It represents average age players, with average potential,
#           who are very strong(as they were younger, they practiced a lot
#           to become stronger, they have a average agility, as they started
#           to play more by thinking, they can control the ball on average
#           as well as dribble, volly and short pass.
#Cluster 3:It represents the oldest players, with average potential,
#           who are average in strength, with very low agility as they
#           became very wise throughout the years and can't take risks,
#           they are bad at controlling the ball, also in dribble,
#           volly and short pass.
#In summary we have three clusters: Good, Average, Bad
# 18. Pick a player from each of those clusters. Describe those players in terms of
# their affiliation to the clusters. What are the similarities and differences between them? (1)
###??????
#Philippe Coutinho
View(Soccer[850,])
View(Soccer[c(850,675),])
View(Soccer[c(850,675,2051),])
View(Soccer[c(850,2051,4499),])
View(data_soccer_club)
View(soccer_data)
View(soccer_data)
View(Soccer)
View(Soccer)
Liverpool<-Soccer$team_long_name==Liverpool
Liverpool<-(Soccer$team_long_name==Liverpool)
Liverpool<-Soccer[(Soccer$team_long_name==Liverpool),]
Liverpool<-subset((Soccer$team_long_name==Liverpool)
Liverpool<-subset(Soccer$team_long_name==Liverpool)
Soccer.<-subset(Soccer,Soccer$team_long_name==Liverpool)
?subset
Soccer.<-subset(Soccer,Soccer$team_long_name=="Liverpool")
View(Soccer.)
Liverpool<-subset(Soccer,Soccer$team_long_name=="Liverpool")
View(Liverpool)
cor(Liverpool$Age,Liverpool$overall_rating)
Boxplot(Liverpool$Age~Liverpool$overall_rating)
boxplot(Liverpool$Age~Liverpool$overall_rating)
boxplot(Liverpool$overall_rating~Liverpool$Age)
plot(Liverpool$Age)
age<-aggregate(Liverpool$overall_rating,by=list(Liverpool$Age), FUN="mean", na.rm=T)
View(age)
View(age)
plot(age)
plot(age, xlabs="age")
plot(age, xlab="age")
plot(age, xlab="age",ylab="overall rating")
plot(age, xlab="age",ylab="overall rating")
years<-aggregate(Liverpool,by=list(yeasr), FUN="mean", na.rm=T)
years<-aggregate(Liverpool[,],by=list(yeasr), FUN="mean", na.rm=T)
years<-aggregate(Liverpool[,],by=list(Liverpool$Age), FUN="mean", na.rm=T)
View(years)
View(years)
Liverpoolage<-subset(Liverpool,Liverpool$Age==19)
Liverpoolage
View(Liverpoolage)
View(Liverpoolage)
age19<-subset(Liverpool,Liverpool$Age==19)
age19<-subset(Liverpool,Liverpool$Age==19)Liverpoolage
age19<-subset(Liverpool,Liverpool$Age==19)
age19<-subset(Liverpool,Liverpool$Age==19)
age20<-subset(Liverpool,Liverpool$Age==20)
age21<-subset(Liverpool,Liverpool$Age==21)
age22<-subset(Liverpool,Liverpool$Age==22)
age23<-subset(Liverpool,Liverpool$Age==23)
age24<-subset(Liverpool,Liverpool$Age==24)
age25<-subset(Liverpool,Liverpool$Age==25)
age26<-subset(Liverpool,Liverpool$Age==26)
age27<-subset(Liverpool,Liverpool$Age==27)
age28<-subset(Liverpool,Liverpool$Age==28)
age29<-subset(Liverpool,Liverpool$Age==29)
age30<-subset(Liverpool,Liverpool$Age==30)
age31<-subset(Liverpool,Liverpool$Age==31)
rown(age19)
nrow(age19)
nrow(age19)
nrow(age19)
nrow(age19)
nrow(age19)
nrow(age19)
nrow(age19)
nrow(age21)
nrow(age22)
nrow(age23)
nrow(age24)
nrow(age25)
nrow(age26)
nrow(age27)
nrow(age28)
nrow(age29)
nrow(age30)
nrow(age31)
years<-aggregate(Liverpool$Age,by=list(Liverpool$Age), FUN="mean", na.rm=T)
plot(Liverpool$Age)
View(years)
View(years)
plot(Liverpool$Age)
hist(Liverpool$Age)
hist(Liverpool$Age)
max(Liverpool$overall_rating)
Which.rows(max(Liverpool$overall_rating))
Which.row(max(Liverpool$overall_rating))
Which.max(Liverpool$overall_rating))
Which.max(Liverpool$overall_rating)
which.max(Liverpool$overall_rating)
Liverpool[which.max(Liverpool$overall_rating),1]
Liverpool[which.min(Liverpool$overall_rating),1]
Liverpool[which.max(Liverpool$overall_rating),5]
scale(Liverpool[,5:42])
Liverpool_scale<-scale(Liverpool[,5:42])
Liverpool_distance<-dist(Liverpool_scale, method="euclidean")
Liverpool_clust<-hclust(Liverpool_distance)
plot(Liverpool_clust, hang=-1)
plot(Liverpool_clust, hang=-1, labels=F)
rect.hclust(Liverpool_clust,3)
plot(Liverpool_clust, hang=-1, labels=F)
rect.hclust(Liverpool_clust,3)
plot(Liverpool_clust, hang=-1, labels=F)
rect.hclust(Liverpool_clust,2)
nrow(Liverpool)
plot(Liverpool_clust, hang=-1, labels=F)
rect.hclust(Liverpool_clust,3)
Liverpool$membership<-cutree(Liverpool_clust,3)
data_Liverpool_<-aggregate(Liverpool[], by=list(Liverpool$membership), FUN="mean", na.rm=T)
data_Liverpool_<-aggregate(Liverpool[,5:42], by=list(Liverpool$membership), FUN="mean", na.rm=T)
View(data_Liverpool_)
library(ggplot2)
library(factoextra)
require(cluster)
fviz_nbclust(GCI[,2:13], hcut, method = "silhouette",hc_method = "complete")
View(GCI)
View(GCI)
cor(GCI[,5]~GCI[,12])
cor(GCI[,5],GCI[,12])
cor(GCI[,6],GCI[,13])
View(GCI)
View(GCI)
plot(GCI[,6],GCI[,13])
plot(GCI[,6],GCI[,13], xlab="Higher education and training",ylab="Innovation")
View(GPI_data)
View(GPI)
